import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image

# Load pre-trained ResNet model
resnet = models.resnet152(pretrained=True)

# Remove the classification layer at the end
modules = list(resnet.children())[:-1]
resnet = nn.Sequential(*modules)

# Freeze the pre-trained layers
for param in resnet.parameters():
    param.requires_grad = False

# Define the captioning model
class CaptioningModel(nn.Module):
    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):
        super(CaptioningModel, self).__init__()
        self.embed = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
        self.linear = nn.Linear(hidden_size, vocab_size)
    
    def forward(self, features, captions):
        embeddings = self.embed(captions[:,:-1]) # remove <end> token
        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)
        hiddens, _ = self.lstm(embeddings)
        outputs = self.linear(hiddens)
        return outputs

# Define image preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])

# Load vocabulary
vocab_size = 10000  # Example vocabulary size
word_to_idx = {}    # Dictionary mapping words to indices
idx_to_word = {}    # Dictionary mapping indices to words

# Define other hyperparameters
embed_size = 256
hidden_size = 512
num_layers = 1

# Initialize model
model = CaptioningModel(embed_size, hidden_size, vocab_size, num_layers)

# Load pre-trained weights if available
# model.load_state_dict(torch.load('path_to_pretrained_weights'))

# Set evaluation mode
model.eval()

# Function to generate caption for a given image
def generate_caption(image_path):
    image = Image.open(image_path)
    image = transform(image).unsqueeze(0) # Add batch dimension
    features = resnet(image).squeeze(0)    # Remove batch dimension
    features = features.unsqueeze(0)       # Add batch dimension
    sampled_ids = []

    states = None
    inputs = features
    for i in range(max_caption_length):
        hiddens, states = model.lstm(inputs, states)
        outputs = model.linear(hiddens.squeeze(1))
        _, predicted = outputs.max(1)
        sampled_ids.append(predicted)
        inputs = model.embed(predicted)
        inputs = inputs.unsqueeze(1)
    sampled_ids = torch.stack(sampled_ids, 1)
    sampled_ids = sampled_ids[0].cpu().numpy() # Convert to numpy array
    sampled_caption = []
    for word_id in sampled_ids:
        word = idx_to_word[word_id]
        sampled_caption.append(word)
        if word == '<end>':
            break
    return ' '.join(sampled_caption)

# Example usage
image_path = 'path_to_image.jpg'
caption = generate_caption(image_path)
print(caption)
